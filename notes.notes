In dataloader.py, splitDataset(). line 167 onwards seems sus??
    Do they only take users that are in the intersection of the two datasets??

    22:05
    Trained in 2 mins, 27 epochs
    Acc@1 26.42%  f1 = 18.14 mrr = 38.19


Valid sequences:
    getValidSequenceUsers() in dataloader.py
    self.previous_day is the minimum length of days in the sequence
    
    DF"st_daY"> row['st_day'] - previous_day
        Only keep the rows within the last 7 days

    Hist should have at least two locations

    Each user's dict: 
        data_dict["X"] = hist["location_id"].values
        data_dict["user_X"] = hist["user_id"].values
        data_dict["start_min_X"] = hist["start_min"].values
        data_dict["mode_X"] = hist["mode"].values
        data_dict["length_X"] = hist["length_m"].values
        data_dict["weekday_X"] = hist["weekday"].values

        # the next location is the Y
        data_dict["loc_Y"] = int(row["location_id"])
        # the next mode is the mode_Y
        data_dict["mode_Y"] = int(row["mode"])

    How does this flow into the MODEL??????

latlong is used for mobtcast kind of model.


WILL EMEET: 23-07
CONVERT DATA TO BE SAME FORMAT
Try with all pois, then 100k pois, then 10k
Try with the same format, 7 days history. 

Ask Josh how to quantify the answers into a smaller set
    Ask if we can see an example survey
One small optoin: given all other data, predict the depression answers
Another, we can predict the emotions. Scale for each emotion, minimize l2 loss
    -translate latlon into poi equivalent

Shoot Hanyu an email asking for SHAP update


Dedicate some time to add some fairness metrics

NOTES:
!!THE CHUNKS MIGHT HAVE BREAKS BETWEEN USERS!!!!

Using 0 as a padding token for users, and 0,1 as padding and unknown tokens for locations.
Loading a full chunk is always useful. Loading a smaller cache chunk takes the same time.

Started a training ruyn for El Paso at 19:48pm 16-08-2024

Getting fails because of nan loss. 

Restarted with some loss logging, at 00:00 17-08-2024
    see utils/train.py

    Removed loss_mode in config.
    This seemed to have avoided the nan, but resulted in an illegal memory access error when doing loss_backward
Cache may be the culprit
Trying with loss_mode, skipping backward with a try - catch
    Started at 11:11 17-08-2024
    Leads to Nans.
    But does not crash at least for 4 epochs

Trying without loss mode see if that helps nans
    Started at 13:56 17-08-2024
    Validation loss in nan after 1 epoch

Adding some logging in the validation loop to see where the nan is coming from
    Started at 15:07 17-08-2024
    Nan is coming from the loss calculation?
    Ran into an error (human error). Rerunning
    16:05
    This also crashed with out of bounds memory access.

Trying without caching.
    Started at 20:52.
    Expecting it to be slower, but should not fail??
    Slows dows significantly after ~4 epochs
    6th epoch took ~6 hours or more.

Its possible the mode embedding is leading to nans
    By default, it assumes 8 modes. So it might be better to not embed mode.

Nans first detected in embedding layer.
Trying custom embedding layer
!!ALSO REDUCED LR. CHECK THIS.!!

Training with El Paso, with much larger model
    batch_size: 256
    embedding:
        base_emb_size: 256 # ws 64
        user_emb_size: 1 # was 16
    model:
        num_encoder_layers: 8 # was 4
        nhead: 8
        dim_feedforward: 1024 #was 512
    
    Batch size 256 (57M parameters) takes ~20GB of VRAM.
    1 epoch takes ~1 hour.

    Final acc@20~11% (Test) afer 21 epochs.

Dallas metro has much higher volumne of data ~80mil data points
Dallas-Tarrant has ~40 million


Implementing new model:
1. Need to implement the mode results dict as cluster dict at some point
2. Think about embedding time to next location
3. Using user embedding based on home latlon 

Structure:
Input: [X, start_min_X, weekday_X, cluster_X, intra_cluster_id_X, user, homelat, homelon, time_to_next]
Output possibilities: [poi_Y, cluster_Y, intra_cluster_id_Y]

Decisions to make:
1. Time to next location can be timedelta, or time of day. One may be better than the other. Think
2. How to use cluster_id and intra_cluster_id together for output? Alone, cluster is fine, but together, the predicion probelm becomes harder.
3. Think about whether home latlon needs normalization
4. Think of a tiered network where the cluster is fed to intra-cluster output layer. How would training be done?

-----------------
Eastland Training
-----------------

Type 1: Location based. (Only use location ID)
1. (start at 11:10)
    ~1.2M params
    With user embedding based on home loc and time to next embedding
    Hyperparams:
        bathc==ch size 128
        emb_size 64
        enc_layers 8
        nhead 8
        dim_feedforward 1024
        lr 1e-4
    Around 2 epochs, acc@20~5.62%
    Got better accuracy at 20 in 5 epochs ~ 11%
    Val accuracy at 10 epoachs: 15.5%
    39 epochs. Failed at testing
    Total time 79 mins
2. Start at 12:34
    900k params only
    With user embedding based on home loc and time to next embedding
    Hyperparams:
        batch size 1024
        emb_size 128
        enc_layers 4
        nhead 8
        dim_feedforward 512
        lr 1e-3
    Around 3 epochs, acc@20~12%
    Much faster, 1 epoch in ~30 seconds
    5 epochs, acc@20~15%
    15 epochs: >16%. The changes seem to be helping, maybe LR?
    Training acc@20 is around 30%, so I guess it is learning to overfit as well?
    Test acc 13.4%
    Total time 9.21 mins

Type 2: Cluster based. (Use cluster ID and intra cluster ID input embeds)
1. Start at 11:30
    1.2M params
    With user embedding based on home loc and time to next embedding
    Hyperparams:
        batch size 1024
        emb_size 128
        enc_layers 4
        nhead 8
        dim_feedforward 512
        lr 1e-3
    Stopped in 16 epochs. Val acc@20~15.5%, train acc~25%
    Total time 8.5 mins
2. Start at 12:45
    Same, but lower initial lr 1e-4
    Just takes more epochs
    Stopped at 61 epochs
    TOtal time 33 mins
    Test acc@20~13%
" NO BENEFIT TO REDUCING LR. Sticking to 1e-3"
" NO PERFORMANCE DIFFERENCE BETWEEN USING RAW LOC AS EMBEDDING VS CLUSTER/INTRACLUSTER IDS"

-----------------
Threshold for improvement with cluster/intracluster ids
-----------------
Goal: Identify if there is a benefit to using cluster/intracluster ids as input embeddings
Hyperparams:
    batch size 1024
    emb_size 128
    enc_layers 4
    nhead 8
    dim_feedforward 512
    lr 1e-3
Harrison County: num pois = 1300:
Model sizes:
1. With location ID only
    1.2 M params
2. With cluster/intracluster ID as input embeds (use_clusters True)
    1.31 M params
3. With cluster/intracluster ID as input and output
    1.41 M params
El Paso County: num pois = 11800:
Model sizes:
1. With location ID only
    4.05 M params
2. With cluster/intracluster ID as input embeds (use_clusters True)
    2.83 M params
3. With cluster/intracluster ID as input and output (predict clusters and predict intracluster)
    1.41 M params

" Benefits are visisble as soon as #pois> #clusters + #intraclusters"
" Main benefit is that the model size for entire texas will be 1.41 M params, as there is no scaling with number of pois"
" --Caveat: If considering the intracluster id to be conditioned on the cluster, the benefit may appear later."

-----------------
Harrison Training
-----------------

Type 1: Location based. (Only use location ID)
1. 1.2 M params
    Failed with Nan after epoch 3
    Takes 30-40 mins to get there

Type 2: Cluster based. (Use cluster ID and intra cluster ID input embeds)
1. 1.31 M params
    Failed with Nan during epoch 3
    Takes 30-40 mins to get there

In both cases, managed to hit ~13% acc@20. 

Issue is in the loss calculation. Testing.
    
2. Started at 14:37
    Added some checks to see where the nan is coming from
    Still fails without triggering any checks

3. Trying again with gradient checks and user norm
    Started at 16:32
    Seems to be working better. Hit epoch 4 around 16:50
    Happened again. There is a nan in logits. 
    if torch.isnan(logits_loc).any() or torch.isinf(logits_loc).any():
        raise ValueError("Logits contain NaN or Inf.")
    Essentially, all logits become Nans. So again the same problem as earlier.

4. Trying with reduced LR (1e-4)
    Started at 17:42
    Still happens at epoch 4.
    Loss value: 6.5895819664001465
        Largest values: torch.return_types.topk(
        values=tensor([[2.2205, 1.8842, 1.7425,  ..., 1.6537, 1.6503, 1.6264],
                [1.9282, 1.8379, 1.8033,  ..., 1.5859, 1.4761, 1.4713],
                [2.1471, 1.9998, 1.9845,  ..., 1.6970, 1.6903, 1.6895],
                ...,
                [1.3840, 1.3534, 1.3250,  ..., 1.1955, 1.1770, 1.1323],
                [6.4891, 6.3816, 6.3551,  ..., 6.2053, 6.1475, 6.0898],
                [1.2057, 1.0088, 0.9772,  ..., 0.8268, 0.8213, 0.7783]],
            device='cuda:0', grad_fn=<TopkBackward0>),
        indices=tensor([[ 288,   55,  210,  ...,  756, 1002, 1336],
                [ 871,   13, 1116,  ...,  343,  405,  685],
                [ 343, 1213,  764,  ..., 1116,  240, 1305],
                ...,
                [ 856,  343,  787,  ...,  871, 1089,  240],
                [1048,  288,  545,  ...,   55,  210, 1188],
                [  32, 1230,  123,  ..., 1123,  447,  799]], device='cuda:0'))
        RuntimeError: Function 'NativeLayerNormBackward0' returned nan values in its 0th output.

5. Trying with no warm up
    Started at 22:23
    This didnt fail at 2 epochs, so warmup is not causing this
    Failed again at epoch 4

6. Trying running on cpu
    May be able to try a very large batch size
    Too slow.. abandoning

7. Batch size 512
    Hit at epoch 1! Got Nans in logits
    Next retry failed at epoch 3

8. Trying with blocking cuda async
    Started at 14:56
    1024 batch size
    LR 0.001
    Got to epoch 14, acc@20 14.10

9. Trying with only 2 encoder layers, 1024 dim

-----------------
El Paso Training
-----------------

Type 1: Location based. (Only use location ID)

Type 2: Cluster based. (Use cluster ID and intra cluster ID input embeds)
With 512 batch size
1. 2.1 M params
    Started at 00:24
    Didnt even get past 1 epoch. Nans in logits, even with lr 1e-4
    Trying again with nhead 4
2. Trying with the safe model
    Started at 18:31
    Batch size 512

-----------------------
Trying a simple encoder model which is not a transformer:

Trained eastland in 10 mins, test acc@20 = 12.99%

Trying el paso
2 M params
It did train in 502 mins, but the acc@20 was 5.07%. Even the training acc@20 was just 8.59%

-----------------------
Trying an LSTM model:
Hidden dim = base_emb_size (128)

600k params for eastland
    Starts to overfit really quickly. Val acc drops from 9% to 2% by epoch 5. Then early stopping kicks in thankfully
    Finished in 18 mins (40 epochs)
    Test acc@20 = 13.54%
    Getting more details
    Test:
    acc@1 0.58 acc@5 = 3.06 acc@10 = 6.29 acc@20 = 12.15

Trying Harrison
    780k params
    Takes 2mins per epoch
    Also immediately starts overfitting. Maybe lr is too high
    Max accuracy isnt all that good, ~11% val top20
    Took 100 epochs
    Test acc@20 10.06%

Trying el paso
    2.3 M params
    In about 10 epochs, loss = 7.56, val acc@20 = 11.75
    Manually stopped

    4096 batch size, 3 lstm layers
    2.4 M params
    Epoch 31, val acc@20~12%
     !An illegal memory access was encountered.
     So it happens even here, may not be just a transformer issue


Trying with predicting clusters only

----LSTM----
Eastland:
    870k params
    4 clusters
    Val Acc@1 hits 76% in 5 epochs (BS 4096)
    6 mins. Test acc@1 = 74.4%, acc@5 = 87.79 acc@10 = 87.95 acc@20 = 88.12
    Does not approach 100% for some reason. It should be easy to.

Harrison:
    Also 870k params (by structure)
    8 clusters.
    Val Acc@1 hits 77% in first epoch.
    in 5 epochs, val acc@1 = 85.19%, acc@20=98.5%
    Took 30 mins
    Test acc@1 = 86.38%, acc@5 = 98.53 acc@10 = 99.22 acc@20 = 99.23

El Paso:
    17 clusters
    870k params (3 layers, as used in previous 2)
    in~20 epochs., val acc@1 ~45%, acc@20=100%
    750 mins
    Test acc@1 = 45.5%, acc@5 = 84.38 acc@10 = 96.07 acc@20 = 100.00


Tiered model, with cluster id being fed to intra_cluster fc layer:
2.07 M params
Eastland
    Let's see if this is more accurate than the previous model
    acc@1 = 0.63 acc@5 = 3.31 acc@10 = 6.63 acc@20 = 12.87
    Similar. Test Acc@20~12.8%.. 10 mins

Harrison
    Took 48 mins.
    Acc@20 = 8.65%, which is worse than the previous model
    acc@1 = 0.40 acc@5 = 2.13 acc@10 = 4.44 acc@20 = 8.65

El Paso
    Took 1357 mins
    Acc@1 1.19% acc@5 = 3.94 acc@10 = 6.47 acc@20 = 10.22
    Slightly worse than previous

Naive tiered prediction:
Eastland
    10 mins
    acc@1 = 0.63 acc@5 = 3.34 acc@10 = 6.68 acc@20 = 12.96
Harrison
    49 mins 
    acc@1 0.40 acc@5 = 2.12 acc@10 = 4.44 acc@20 = 8.65
El Paso
    Failed with illegal memory access
    Val accs:
    acc@1 0.83 acc@5 = 3.13 acc@10 = 5.15 acc@20 = 8.18

Adding hidden dim using dim_feedforward:
Changed LSTM layers to 2
Predicting pois
Harrison:
With 1024:
    22 M params
With 512:
    4.7 Mil
    Ran into illegal instruction was encountered?? (Epoch 5)
Trying without time embed
With 512:
    Lower accuracy in val (~10 vs ~12 earlier). Getting higher though. Got to 11.7 by epoch 34
    Just randomly died??? No error, jsut stopped during training
    retrying:
    Stops at epoch 17
    Test acc@1 = 0.71 acc@5 = 3.17 acc@10 = 5.81 acc@20 = 10.55
With intra-clusters being predicted, using time embed:
    Needed to reduce bs to 1024


New LSTM sizes:
With dim_feedforward 512
Eastland:
    6150388 6M  is quite large.. reducing dim dim_feedforward to 256
With dim_feedforward 256
Eastland:
    2.9M params

New eval results:
Eastland LSTM, patience 1, bs 1024, dim_feedforward 512:
6M params
    1. Predicting latlon
    Training finished.       Time: 3.73min.  acc@1: 0.72%
    acc@1 = 0.50 f1 = 0.23 mrr = 2.83
    acc@5 = 2.66 acc@10 = 5.42 acc@20 = 10.53
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.364770  17.303196  31.332814  49.337490
    1        2w  6.547155  24.318005  41.621200  60.483242
    2        1m  8.963367  32.112237  52.299299  70.303975

    2. Predicting clusters+intra_cluster_id
    Training finished.       Time: 3.30min.  acc@1: 0.82%
    acc@1 = 0.65 f1 = 0.00 mrr = -0.10
    acc@5 = 3.50 acc@10 = 6.75 acc@20 = 12.67
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.442712  18.550273  31.332814  50.194856
    1        2w   6.703040  26.578332  43.725643  63.367108
    2        1m  10.054560  36.087295  55.572876  74.902572

    Pretty good tbh. Definitely comparable to Hanyu's results now

Eastland LSTM, patience 1, bs 1024, dim_feedforward 256:
BS 4096 with Eastland is wasteful, it just adds to the number of epochs
2.9M params
    1. Predicting cid+intra_cid
    11 epochs
    Training finished.       Time: 5.07min.  acc@1: 0.85%
    acc@1 = 0.68 f1 = 0.00 mrr = -0.10
    acc@5 = 3.44 acc@10 = 6.70 acc@20 = 12.76
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.364770  20.265004  33.515199  54.715511
    1        2w  6.469213  28.137178  44.660951  67.965705
    2        1m  9.664848  38.425565  56.586126  77.942323

    
Harrison LSTM, patience 1, bs 4096 dim_feedforward 256:
This also learns faster with BS 1024 (fewer epochs) but larger BS should be better (?)
2.9M params
    2. Predicting clusters+intra_cluster_id
    9 epochs
    Training finished.       Time: 13.92min.         acc@1: 0.62%
    acc@1 = 0.44 f1 = 0.00 mrr = -0.03
    acc@5 = 2.16 acc@10 = 4.36 acc@20 = 8.49
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.563319  17.772926  29.039301  44.934498
    1        2w  6.965066  25.807860  40.982533  58.799127
    2        1m  9.672489  34.213974  51.943231  69.803493
    This is not great, can do better (have done better, but with 512 hidden dim?)

El Paso LSTM, patience 1, bs 1024 dim_feedforward 256:
2.9M params
    1. Predicting tiered output (cid+intra_cid)
    Training finished.       Time: 444.57min.        acc@1: 1.34%
    acc@1 = 1.17 f1 = 0.00 mrr = -0.10
    acc@5 = 4.20 acc@10 = 6.81 acc@20 = 10.52
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.448496  12.000000  17.979945  26.359161
    1        2w   7.030082  18.067457  26.089335  36.678213
    2        1m  10.661805  25.538742  35.631723  47.544211


Harrison full:
With patience 3, bs 1024, dim_feedforward 512:
    Training finished.       Time: 40.53min.         acc@1: 0.72%
    acc@1 = 0.51 f1 = 0.00 mrr = -0.10
    acc@5 = 2.54 acc@10 = 4.71 acc@20 = 8.79
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.061135  17.816594  29.170306  45.480349
    1        2w  6.441048  26.026201  41.244541  59.934498
    2        1m  9.475983  35.021834  52.816594  71.375546
    Not a significant improvement over the previous model

!Transformer:
Added max_seq_length to dataloader. Trying with 100, see if transformer fails:
BS 1024, dim_feedforward 512, max seq length 100
Eastland:
    Training finished.       Time: 16.99min.         acc@1: 0.77%
    acc@1 = 0.64 f1 = 0.00 mrr = -0.10
    acc@5 = 3.48 acc@10 = 6.75 acc@20 = 12.87
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   5.222136  18.550273  30.865160  47.934528
    1        2w   7.716290  25.798909  42.400624  60.561185
    2        1m  11.223694  35.229930  52.844895  71.551052
Harrison:
    predicting tiered
    2.5M params
    Its still going at 22 epochs, so maybe there is hope!
    Worked
    Training finished.       Time: 62.87min.         acc@1: 1.01%
    acc@1 = 0.79 f1 = 0.00 mrr = -0.10
    acc@5 = 3.69 acc@10 = 6.36 acc@20 = 11.28
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.104803  16.157205  27.139738  43.362445
    1        2w  6.768559  24.082969  38.733624  56.855895
    2        1m  9.672489  32.947598  50.196507  68.253275

Trying El Paso:
    Got to 6 epochs in about 2-3 hours.
    Ended at 34 epochs 
    Training finished.       Time: 1059.96min.       acc@1: 1.64%
    acc@1 = 1.43 f1 = 0.00 mrr = -0.10
    acc@5 = 4.80 acc@10 = 7.59 acc@20 = 11.38
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.331814  11.595260  17.341841  25.290793
    1        2w   6.913400  17.571559  25.228806  35.463993
    2        1m  10.756609  25.265269  34.946217  46.264357
    "Results look similar to the LSTM model, even slightly worse."
    "Worse might be because of the limit on sequence length."

Trying with 200 seq length:
Eastland:
    Training finished.       Time: 17.00min.         acc@1: 0.77%
    acc@1 = 0.64 f1 = 0.00 mrr = -0.10
    acc@5 = 3.48 acc@10 = 6.75 acc@20 = 12.87
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   5.222136  18.550273  30.865160  47.934528
    1        2w   7.716290  25.798909  42.400624  60.561185
    2        1m  11.223694  35.229930  52.844895  71.551052
Harrison:
    Training finished.       Time: 83.18min.         acc@1: 0.85%
    acc@1 = 0.71 f1 = 0.00 mrr = -0.10
    acc@5 = 3.27 acc@10 = 6.04 acc@20 = 10.66
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.126638  16.790393  28.995633  44.934498
    1        2w  6.462882  24.454148  40.152838  57.947598
    2        1m  9.497817  33.296943  51.310044  69.170306
El Paso:
    Got nans in 2st epoch. So 200 max len is too much. 
    Trying 150
    Nans in 1st epoch again.
    Trying half the batch size. 512
    Same... Why?
    Trying with 100 max seq length, BS 1024. This worked earlier.
        This seems to still work
        Weird.

Lab PC Training:
BS 4096, dim_feedforward 512, max seq length 100
Eastland test:
    100 max length. Also changed the max length of the eval. 
    41 epochs
    Training finished.       Time: 7.40min.  acc@1: 0.79%
    acc@1 = 0.67 f1 = 0.00 mrr = -0.03
    acc@5 = 3.03 acc@10 = 6.21 acc@20 = 12.26
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  4.364770  19.485581  35.307872  57.911146
    1        2w  6.936867  27.669525  46.453624  70.459860
    2        1m  9.508963  35.775526  59.314108  80.826189
    Significantly better numbers here. Let's try Harrison.

Harrison test:
    Training finished.       Time: 58.02min.         acc@1: 0.60%
    acc@1 = 0.45 f1 = 0.00 mrr = -0.03
    acc@5 = 2.20 acc@10 = 4.40 acc@20 = 8.48
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.388646  17.751092  30.851528  47.860262
    1        2w   6.899563  25.960699  43.209607  62.882096
    2        1m  10.109170  34.847162  54.563319  73.995633

El Paso test:
    This should be better again
    Started at 6:58pm, 11/12



Eastland Unfiltered:
Without removing pois outside the county
    Training finished.       Time: 16.53min.         acc@1: 0.61%
    acc@1 = 0.50 f1 = 0.00 mrr = -0.03
    acc@5 = 2.28 acc@10 = 4.12 acc@20 = 7.38
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.491726  16.784870  28.999212  44.680851
    1        2w   6.225374  24.271080  39.085894  55.870764
    2        1m  10.323089  34.830575  50.039401  66.036249
    "Takes longer, is less accurate"
Harrison Unfiltered:
    38 epochs
    Training finished.       Time: 82.67min.         acc@1: 1.03%
    acc@1 = 0.82 f1 = 0.00 mrr = -0.02
    acc@5 = 3.00 acc@10 = 4.99 acc@20 = 8.08
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.709083  15.006279  24.926748  37.128506
    1        2w   8.078694  22.938468  36.102972  51.151109
    2        1m  11.825031  32.544998  47.321055  62.494768

El Paso:
    100 epochs (lab PC) Took about 21 hoours
    acc@1 = 1.36 f1 = 0.00 mrr = -0.02
    acc@5 = 4.61 acc@10 = 7.35 acc@20 = 11.15
    Eval results:
    lookahead      acc@1      acc@5     acc@10     acc@20
    0        1w   4.255242  11.806746  17.845032  26.096627
    1        2w   6.698268  17.742935  25.969006  36.510483
    2        1m  10.362808  25.378304  35.646308  47.416591

    Started training on the cluster with BS 12288. at 12/12/24, 3:26pm. Should be 3X as fast.
        Check back in a bit to verify.
        4:38 pm 5 epochs in 1 hour. Not really faster, then. Maybe the bottleneck is the data loading.
        
        Got some okay intermediate results:
        Epoch 10, 98.8%  loss: 8.044 acc@1: 1.13 f1: 0.00 mrr: -0.01, took: 20.66s, acc@5: 4.34, acc@10: 7.01, acc@20: 11.13 
        Validation loss = 8.22 acc@1 = 1.10 f1 = 0.00 mrr = -0.01
        acc@5 = 3.96 acc@10 = 6.45 acc@20 = 10.09
        Eval results:
        lookahead      acc@1      acc@5     acc@10     acc@20
        0        1w   4.153145  12.886053  19.952598  29.152233
        1        2w   6.654512  19.522334  28.816773  40.386509
        2        1m  10.187785  27.722881  39.226983  51.584321
        
        # seems to get worse when finishing. 
        98 epochs
        Training finished.       Time: 1442.24min.       acc@1: 1.43%
        acc@1 = 1.26 f1 = 0.00 mrr = -0.01
        acc@5 = 4.39 acc@10 = 7.02 acc@20 = 10.74
        Eval results:
        lookahead      acc@1      acc@5     acc@10     acc@20
        0        1w   4.182315  11.810392  18.096627  26.348222
        1        2w   6.778487  17.914312  26.329991  36.689152
        2        1m  10.512306  25.786691  35.854148  47.460346
        "Still took the entire day, not any faster"


A 100 sequence length preprocessing:
!Doing a 100 sequence length preprocessing instead of 7 days does weird things
    Eastland: drops some users. Accuracy Top20 is now lower (54% instead of 71%)
    ! This may be because of not dropping users with <3 data points in the 1m lookahead data
    !!!TEST!!! - Seems to be correct

    Harrison: 
    This also drops in quality. Top20 is now 50% instead of 74%
    Training finished.       Time: 60.90min.         acc@1: 1.14%
    acc@1 = 0.97 f1 = 0.00 mrr = -0.02
    acc@5 = 4.33 acc@10 = 7.78 acc@20 = 13.72
    Eval results:
    lookahead     acc@1      acc@5     acc@10     acc@20
    0        1w  3.171475  12.721734  21.716538  33.596130
    1        2w  4.945350  18.419638  30.012543  42.931374
    2        1m  6.952159  24.690916  37.627665  50.707758

    El Paso:



Try Dallas with 100.

TODO: Train the tiered embedding separately.
! BIG TODO: Train without the 7 day previous window. 
    Generate these datasets.
    !Convert to max_seq_length instead of previous_days
! Pick a lane: Either dont filter from the dataset for the eval lookahead data, or do.
!BIG TODO: Test the evaluation accuracy functions, with some sample inputs. Make sure no bugs
Idea: Positional embed the raw poi embeds based on latlon?