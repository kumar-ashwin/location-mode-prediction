misc:
  # #ElPaso
  # dataset: 'El_Paso'
  # # 0 reserved for padding, 1 reserved for unknown 
  # total_loc_num: 11796
  # # 0 reserved for padding
  # total_user_num: 42692
  
  #Harrison
  dataset: 'Harrison'
  total_loc_num: 1369
  total_user_num: 69050

  # #Eastland
  # dataset: 'Eastland'
  # # 0 reserved for padding, 1 reserved for unknown
  # total_loc_num: 525
  # # 0 reserved for padding
  # total_user_num: 1922

  total_cluster_num: 1060
  max_intra_cluster_num: 1000

  predict_clusters: True
  predict_intra_cluster: True
  use_clusters: True

  # 
  if_embed_loc: True
  if_embed_user: True
  if_embed_time: True

  # how many days in the past we consider
  previous_day: 7
  verbose: True
  debug: False
  batch_size: 1024
  # batch_size: 4096
  print_step: 5
  

embedding:
  # also size for time and mode
  # base_emb_size: 64
  base_emb_size: 128
  # base_emb_size: 256
  # user_embedding added at the end 
  # user_emb_size: 16
  user_emb_size: 8
  

model:
  # networkName: lstm/ mlp/ transformer
  networkName: lstm
  # tuned 
  num_encoder_layers: 4
  # num_encoder_layers: 8
  nhead: 4
  # nhead: 8
  dim_feedforward: 512
  # dim_feedforward: 1024
  dropout: 0.1
  fc_dropout: 0.1

optimiser:
  optimizer: Adam
  max_epoch: 100
  lr: 0.001
  weight_decay: 0.000001
  # for Adam
  beta1: 0.9
  beta2: 0.999
  # for SGD
  momentum: 0.98
  # for warmup
  num_warmup_epochs: 2
  num_training_epochs: 50
  # for learning rate decay with early stop
  patience: 3
  lr_step_size: 1
  lr_gamma: 0.1

dataset:
  source_root: ./data/
  save_root: ./outputs/
